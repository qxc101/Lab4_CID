{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBdiD49e5Q7L"
      },
      "source": [
        "# **Lab 4: Adversarial Attacks Against Machine Learning Based SpamFilters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPJYb7EA5a24"
      },
      "source": [
        "Please Type the Names of the Team Members:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSubQvScyKdQ"
      },
      "source": [
        "## **Introduction**\n",
        "Machine learning-based spam detection models learn from a set of labeled training data and detect spam emails after the training phase. We study a class of vulnerabilities of such detection models, where the attack can manipulate a trained model, e.g., a SVM classifier, to misclassify maliciously crafted spam emails at the detection phase. However, very often feature extraction methods make it very difficult to translate the change in the feature space to that in the textual email space. This lab uses a new attack method of making guided changes to text data by taking advantage of generated adversarial examples that purposely modify the TF-IDF vetor representing an email. We identify a set of \"magic words\", or malicious words, to be added to a spam email, which can cause desirable misclassifications by classifiers.\n",
        "\n",
        "For more information on this method, you can refer to the following publications:\n",
        "\n",
        "(1) J. He, Q. Cheng, and X. Li, “Understanding the Impact of Bad Words \n",
        "on Email Management through Adversarial Machine Learning,” SIG-KM International Research Symposium 2021, Virtual Event, The University of North Texas, September 29, 2021. [Download](https://isi.jhu.edu/wp-content/uploads/2021/10/Bad-Words-He-Cheng-Li-Rev.pdf)\n",
        "\n",
        "(2) C. Wang, D. Zhang, S. Huang, X. Li, and L. Ding, “Crafting Adversarial Email Content against Machine Learning Based Spam Email Detection,” In Proceedings of the 2021 International Symposium on Advanced Security on Software and Systems (ASSS ’21) with AsiaCCS 2021, Virtual Event, Hong Kong, June 7, 2021. [Download](https://isi.jhu.edu/wp-content/uploads/2021/04/ASSS_Workshop_Paper.pdf\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv-y6Ac6FYWO"
      },
      "source": [
        "## **1. Loading Dataset**\n",
        "The dataset we will be using is called Ling-Spam. The Ling-Spam dataset is a collection of 2,893 spam and non-spam messages curated from the Linguist List. These messages focus on linguistic interests around job postings, research opportunities and software discussion.\n",
        "### Acknowledgements\n",
        "All acknowledgements go to the original authors of A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists. The dataset was made publicly available as a part of that paper. \\\\\n",
        "**Run the code block below:**\n",
        "\n",
        "(choose the message.csv to upload. Wait until it shows 100% before you continue.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXkAvUlp4fRP",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "96610ed8-7033-4cdc-d470-613c049f2cf1"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d22ad94c-e601-4a5c-8ceb-2e699bcfd4dc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d22ad94c-e601-4a5c-8ceb-2e699bcfd4dc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving messages.csv to messages.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1re2gZWrle_P"
      },
      "source": [
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZvxGJk-rkkc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def data_extraction():\n",
        "  # change the 'message.csv' to the file names you uploaded\n",
        "  df = pd.read_csv('messages.csv')\n",
        "  x = df.message\n",
        "  y = df.label\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=99)\n",
        "  return x_train, x_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the code section below, we extracted the dataset. \\\\\n",
        "**Run the code block below:**"
      ],
      "metadata": {
        "id": "inXlTIXTP6eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = data_extraction()\n",
        "print(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP6CIbWYP51C",
        "outputId": "93022877-137f-4eeb-990d-e37a321caca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2501    call for squibs > from lisa cheng and rint syb...\n",
            "2294    hello , my name is kevin elphick and i am the ...\n",
            "2204    greetings to the list , i wish to thank everyo...\n",
            "2628    editor 's note : we recently posted informatio...\n",
            "1579    call for participation a workshop on minimizin...\n",
            "                              ...                        \n",
            "1092    this is a one time mailing , if you are not in...\n",
            "1768    / / / / / / / / / / / / / / / / / / / / / / / ...\n",
            "1737    this does n't quite qualify , but ' overlook '...\n",
            "1209    a colloquium on translation is being proposed ...\n",
            "641     call for participation epia ' 97 8th portugues...\n",
            "Name: message, Length: 2314, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFUv7AmsFW8e"
      },
      "source": [
        "In the code block above, we have read the dataset into variables x \n",
        "and y. Variable x contains the email messages and variable y contains\n",
        " the class labels with 0 being ham and 1 being spam. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KJ7iGauJih3"
      },
      "source": [
        "## **2. Preprocessing the Emails**\n",
        "For the emails we used, we need to removed all the HTML tags, numbers, punctuation marks, and English stop words in ordser to keep only useful information. We also need to converted all the words to their lowercase forms and each paragraph into a single line instead of multiple lines. In the last step of data preprocessing, we will conduct stemming on all the words. \\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfS6VpTaH7Wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a1fdd4-5bc3-4398-914f-f1cd9964ca7b"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def remove_hyperlink(word):\n",
        "  return re.sub(r\"http\\S+\", \" \", word)\n",
        "\n",
        "\n",
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', ' ', word)\n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_whitespace(word):\n",
        "    result = word.strip()\n",
        "    return result\n",
        "\n",
        "\n",
        "def replace_newline(word):\n",
        "    return word.replace('\\n', ' ')\n",
        "\n",
        "\n",
        "def clean_up_pipeline(sentence):\n",
        "    cleaning_utils = [remove_hyperlink,replace_newline,to_lower,\n",
        "                      remove_number,\n",
        "                      remove_punctuation,\n",
        "                      remove_whitespace]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_stop_words(words):\n",
        "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
        "    return result\n",
        "\n",
        "\n",
        "def word_stemmer(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(o) for o in words]\n",
        "\n",
        "\n",
        "def word_lemmatizer(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(o) for o in words]\n",
        "\n",
        "\n",
        "def clean_token_pipeline(words):\n",
        "    cleaning_utils = [remove_stop_words, word_lemmatizer]\n",
        "    for o in cleaning_utils:\n",
        "        words = o(words)\n",
        "    return words\n",
        "\n",
        "\n",
        "def preprocess(x_train, x_test):\n",
        "    x_train = [clean_up_pipeline(o) for o in x_train]\n",
        "    x_test = [clean_up_pipeline(o) for o in x_test]\n",
        "    x_train = [word_tokenize(o) for o in x_train]\n",
        "    x_test = [word_tokenize(o) for o in x_test]\n",
        "    x_train = [clean_token_pipeline(o) for o in x_train]\n",
        "    x_test = [clean_token_pipeline(o) for o in x_test]\n",
        "    x_train = [\" \".join(o) for o in x_train]\n",
        "    x_test = [\" \".join(o) for o in x_test]\n",
        "    return x_train, x_test\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the code section below, we preprocessed the dataset. \\\\\n",
        "**Run the code block below:**"
      ],
      "metadata": {
        "id": "prHU5gB1Q_V-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoPue9h3Ik--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560c1ad2-7599-4a9e-d13d-9df3966b607b"
      },
      "source": [
        "x_train, x_test = preprocess(x_train, x_test)\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "squib lisa cheng rint sybesma editor glot international year glot international start featuring squib section invite everybody send squib subject field theoretical linguistics appear monthly production time relatively short able publish squib soon acceptance review procedure set geared losing little time possible squib squib squib inspire present idea fleshed time connection fact thought related spell beginning new analysis necessarily daring new fact old language old fact new guise come beautiful observation theoretically relevant tell wonderful problem possibly hint solution length page glot international word including reference interested submitting squib send hard copy soft copy address sending consult guideline author web site www hagpub com glot htm send email prefer receive guideline email regular mail address email glot rullet leidenuniv nl regular mail lisng rint sybesma glot international department general linguistics leiden university p o box ra leiden netherlands lisa cheng rint sybesma editor glot international hil department general linguistics leiden university po box ra leiden netherlands fax http www hagpub com glot htm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N68YIZqIJmc8"
      },
      "source": [
        "## **3. Feature Extraction**\n",
        "In this step, we convert the text content of an email into a numerical feature vector, representing information of that email used for classification. There are many vectorization methods to convert text data into numerical vectors. In this lab, we will use TF-IDF, modified Word2vec, and Modified Doc2vec to do feature extraction of all the words in an email. \\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlpwcsgxJrMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7299d4a6-03bc-419f-9014-f0cf6de91eb1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Doc2Vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "def convert_to_feature(raw_tokenize_data):\n",
        "    raw_sentences = [' '.join(o) for o in raw_tokenize_data]\n",
        "    return vectorizer.transform(raw_sentences)\n",
        "\n",
        "\n",
        "def TfidfConvert(x_train, x_test):\n",
        "    x_train = [o.split(\" \") for o in x_train]\n",
        "    x_test = [o.split(\" \") for o in x_test]\n",
        "    raw_sentences = [' '.join(o) for o in x_train]\n",
        "    vectorizer.fit(raw_sentences)\n",
        "    x_train_features = convert_to_feature(x_train)\n",
        "    x_test_features = convert_to_feature(x_test)\n",
        "    return x_train_features, x_test_features\n",
        "\n",
        "\n",
        "def getUniqueWords(allWords):\n",
        "    uniqueWords = []\n",
        "    for i in allWords:\n",
        "        if i not in uniqueWords:\n",
        "            uniqueWords.append(i)\n",
        "    return uniqueWords\n",
        "\n",
        "\n",
        "def input_split(x):\n",
        "    new_x = []\n",
        "    for line in x:\n",
        "        newline = line.split(' ')\n",
        "        new_x.append(newline)\n",
        "    return new_x\n",
        "\n",
        "\n",
        "def getUniqueWords(allWords):\n",
        "    uniqueWords = []\n",
        "    for i in allWords:\n",
        "        if i not in uniqueWords:\n",
        "            uniqueWords.append(i)\n",
        "    return uniqueWords\n",
        "\n",
        "\n",
        "def x2vec(input_x, feature_names, model):\n",
        "    x_features = []\n",
        "    for index in input_x:\n",
        "        model_vector = [0] * len(feature_names)\n",
        "\n",
        "        for token in index:\n",
        "            if token in feature_names:\n",
        "                feature_index = feature_names.index(token)\n",
        "\n",
        "                if model.wv.has_index_for(token):\n",
        "                    token_vecs = model.wv.get_vector(token)\n",
        "                    model_vector[feature_index] = token_vecs[0]\n",
        "        x_features.append(model_vector)\n",
        "    return x_features\n",
        "\n",
        "\n",
        "def single_transform(x, method, feature_model, feature_names, scaler, selection_model):\n",
        "    if method == 'TFIDF':\n",
        "\n",
        "        result = feature_model.transform(x)\n",
        "        if selection_model != 'NaN':\n",
        "            result = selection_model.transform(result)\n",
        "        return result\n",
        "    else:\n",
        "        temp_x = x.values\n",
        "        temp_x = temp_x[0].split(' ')\n",
        "        model_vector = [0] * len(feature_names)\n",
        "        for token in temp_x:\n",
        "            if token in feature_names:\n",
        "                feature_index = feature_names.index(token)\n",
        "                if feature_model.wv.has_index_for(token):\n",
        "                    token_vecs = feature_model.wv.get_vector(token)\n",
        "                    model_vector[feature_index] = token_vecs[0]\n",
        "        x_features = [model_vector]\n",
        "        x_features = scaler.transform(x_features)\n",
        "        x_train_features = sparse.csr_matrix(x_features)\n",
        "        if selection_model != 'NaN':\n",
        "            x_train_features = selection_model.transform(x_train_features)\n",
        "        return x_train_features\n",
        "\n",
        "\n",
        "def feature_extraction(x_train, x_test, method):\n",
        "\n",
        "    if method == 'TFIDF':\n",
        "        x_train_features, x_test_features = TfidfConvert(x_train, x_test)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        return x_train_features, x_test_features, feature_names, vectorizer, 'NaN'\n",
        "\n",
        "    if method == 'word2vec':\n",
        "        temp_x_train = input_split(x_train)\n",
        "        temp_x_test = input_split(x_test)\n",
        "        model_train = Word2Vec(temp_x_train, vector_size=1)\n",
        "        feature_space = []\n",
        "        for index in temp_x_train:\n",
        "            feature_space = feature_space + getUniqueWords(index)\n",
        "        feature_names = getUniqueWords(feature_space)\n",
        "        x_train_features = x2vec(temp_x_train, feature_names, model_train)\n",
        "        x_test_features = x2vec(temp_x_test, feature_names, model_train)\n",
        "        x_train_features = np.array(x_train_features)\n",
        "        x_test_features = np.array(x_test_features)\n",
        "        pd.DataFrame(x_train_features).to_csv(\"x_train_features.csv\", header=None, index=False)\n",
        "        pd.DataFrame(x_test_features).to_csv(\"x_test_features.csv\", header=None, index=False)\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(x_train_features)\n",
        "        x_train_features = scaler.transform(x_train_features)\n",
        "        x_test_features = scaler.transform(x_test_features)\n",
        "        x_train_features = sparse.csr_matrix(x_train_features)\n",
        "        x_test_features = sparse.csr_matrix(x_test_features)\n",
        "        return x_train_features, x_test_features, feature_names, model_train, scaler\n",
        "\n",
        "    if method == 'doc2vec':\n",
        "        temp_x_train = input_split(x_train)\n",
        "        temp_x_test = input_split(x_test)\n",
        "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(temp_x_test)]\n",
        "        model_train = Doc2Vec(documents, vector_size=1)\n",
        "        feature_space = []\n",
        "        for index in temp_x_train:\n",
        "            feature_space = feature_space + getUniqueWords(index)\n",
        "        feature_names = getUniqueWords(feature_space)\n",
        "        x_train_features = x2vec(temp_x_train, feature_names, model_train)\n",
        "        x_test_features = x2vec(temp_x_test, feature_names, model_train)\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(x_train_features)\n",
        "        x_train_features_scaled = scaler.transform(x_train_features)\n",
        "        x_test_features_scaled = scaler.transform(x_test_features)\n",
        "        x_train_features = sparse.csr_matrix(x_train_features_scaled)\n",
        "        x_test_features = sparse.csr_matrix(x_test_features_scaled)\n",
        "        return x_train_features, x_test_features, feature_names, model_train, scaler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 46349)\t0.02621363735628304\n",
            "  (0, 46104)\t0.044243412442949326\n",
            "  (0, 45930)\t0.023335189665615534\n",
            "  (0, 45897)\t0.05871887896488733\n",
            "  (0, 45259)\t0.026876570150263467\n",
            "  (0, 43511)\t0.03332411093642023\n",
            "  (0, 41716)\t0.06961103844466418\n",
            "  (0, 41592)\t0.03875217458178802\n",
            "  (0, 41445)\t0.059961608008199815\n",
            "  (0, 41444)\t0.031870003742657305\n",
            "  (0, 41069)\t0.03690291355772966\n",
            "  (0, 40436)\t0.18193988855449547\n",
            "  (0, 39890)\t0.04905362530462098\n",
            "  (0, 39851)\t0.027435748736838014\n",
            "  (0, 39317)\t0.03730012147404211\n",
            "  (0, 39167)\t0.540230402858367\n",
            "  (0, 38954)\t0.059961608008199815\n",
            "  (0, 38657)\t0.03603358217054158\n",
            "  (0, 38594)\t0.0457128349501382\n",
            "  (0, 38519)\t0.06138243821681908\n",
            "  (0, 38143)\t0.028945471752544888\n",
            "  (0, 37728)\t0.03540022191725924\n",
            "  (0, 37401)\t0.03247438138722991\n",
            "  (0, 37251)\t0.04228239275638995\n",
            "  (0, 37244)\t0.06912511215074625\n",
            "  :\t:\n",
            "  (0, 14601)\t0.07996732569784008\n",
            "  (0, 14313)\t0.029211044385985103\n",
            "  (0, 14114)\t0.05710524903351722\n",
            "  (0, 14056)\t0.02092300773253752\n",
            "  (0, 13822)\t0.1009938832123625\n",
            "  (0, 13364)\t0.051836708125756616\n",
            "  (0, 12272)\t0.054258408748113475\n",
            "  (0, 11836)\t0.07367646022482556\n",
            "  (0, 10002)\t0.05260497386239564\n",
            "  (0, 9423)\t0.06905934299426124\n",
            "  (0, 8498)\t0.051817559130713314\n",
            "  (0, 8214)\t0.0480075140474558\n",
            "  (0, 8068)\t0.03956647280966141\n",
            "  (0, 7521)\t0.029796170572767094\n",
            "  (0, 7506)\t0.0550221258632684\n",
            "  (0, 6621)\t0.11992321601639963\n",
            "  (0, 4966)\t0.06553905775572093\n",
            "  (0, 3857)\t0.040928725422177785\n",
            "  (0, 3784)\t0.047810896261308494\n",
            "  (0, 3042)\t0.028081546776456995\n",
            "  (0, 2121)\t0.040361206747972854\n",
            "  (0, 1594)\t0.0275110629316342\n",
            "  (0, 502)\t0.04098249909302187\n",
            "  (0, 228)\t0.03562588785254306\n",
            "  (0, 114)\t0.03512516627427293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the code section below, we extracted the TFIDF values of all the emails in the dataset. \\\\\n",
        "**Run the code block below:**"
      ],
      "metadata": {
        "id": "Cn5SDg6FRJxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method = \"TFIDF\"\n",
        "x_train_features, x_test_features, feature_names, feature_model, scalar = feature_extraction(x_train, x_test, method)\n",
        "print(x_train_features[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzIT4gP1RWX4",
        "outputId": "c2a1808b-4cd5-4d0d-dfba-8310bb19e471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 46349)\t0.02621363735628304\n",
            "  (0, 46104)\t0.044243412442949326\n",
            "  (0, 45930)\t0.023335189665615534\n",
            "  (0, 45897)\t0.05871887896488733\n",
            "  (0, 45259)\t0.026876570150263467\n",
            "  (0, 43511)\t0.03332411093642023\n",
            "  (0, 41716)\t0.06961103844466418\n",
            "  (0, 41592)\t0.03875217458178802\n",
            "  (0, 41445)\t0.059961608008199815\n",
            "  (0, 41444)\t0.031870003742657305\n",
            "  (0, 41069)\t0.03690291355772966\n",
            "  (0, 40436)\t0.18193988855449547\n",
            "  (0, 39890)\t0.04905362530462098\n",
            "  (0, 39851)\t0.027435748736838014\n",
            "  (0, 39317)\t0.03730012147404211\n",
            "  (0, 39167)\t0.540230402858367\n",
            "  (0, 38954)\t0.059961608008199815\n",
            "  (0, 38657)\t0.03603358217054158\n",
            "  (0, 38594)\t0.0457128349501382\n",
            "  (0, 38519)\t0.06138243821681908\n",
            "  (0, 38143)\t0.028945471752544888\n",
            "  (0, 37728)\t0.03540022191725924\n",
            "  (0, 37401)\t0.03247438138722991\n",
            "  (0, 37251)\t0.04228239275638995\n",
            "  (0, 37244)\t0.06912511215074625\n",
            "  :\t:\n",
            "  (0, 14601)\t0.07996732569784008\n",
            "  (0, 14313)\t0.029211044385985103\n",
            "  (0, 14114)\t0.05710524903351722\n",
            "  (0, 14056)\t0.02092300773253752\n",
            "  (0, 13822)\t0.1009938832123625\n",
            "  (0, 13364)\t0.051836708125756616\n",
            "  (0, 12272)\t0.054258408748113475\n",
            "  (0, 11836)\t0.07367646022482556\n",
            "  (0, 10002)\t0.05260497386239564\n",
            "  (0, 9423)\t0.06905934299426124\n",
            "  (0, 8498)\t0.051817559130713314\n",
            "  (0, 8214)\t0.0480075140474558\n",
            "  (0, 8068)\t0.03956647280966141\n",
            "  (0, 7521)\t0.029796170572767094\n",
            "  (0, 7506)\t0.0550221258632684\n",
            "  (0, 6621)\t0.11992321601639963\n",
            "  (0, 4966)\t0.06553905775572093\n",
            "  (0, 3857)\t0.040928725422177785\n",
            "  (0, 3784)\t0.047810896261308494\n",
            "  (0, 3042)\t0.028081546776456995\n",
            "  (0, 2121)\t0.040361206747972854\n",
            "  (0, 1594)\t0.0275110629316342\n",
            "  (0, 502)\t0.04098249909302187\n",
            "  (0, 228)\t0.03562588785254306\n",
            "  (0, 114)\t0.03512516627427293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MND-TjQRXkpq"
      },
      "source": [
        "### **Question 1**\n",
        "Look up the information of Word2vec and Doc2vec online and describe what it does in your own words using one paragraph. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAbh0HSVrCQd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0RZeCBFLSH9"
      },
      "source": [
        "## **4. Training SVM**\n",
        "In this section, we will train a Support Vector Machine (SVM) as an spam filter. \\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAeBdHNfLM7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a80b65-65a3-4bd6-e4d5-3ebae291c746"
      },
      "source": [
        "!pip install secml\n",
        "from secml.data import CDataset\n",
        "from secml.data.splitter import CDataSplitterKFold\n",
        "from secml.ml.classifiers import CClassifierSVM\n",
        "from secml.ml.peval.metrics import CMetricAccuracy\n",
        "from secml.ml.peval.metrics import CMetricConfusionMatrix\n",
        "from secml.adv.attacks.evasion import CAttackEvasionPGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from Feature_extraction import single_transform\n",
        "import csv\n",
        "from statistics import mean, stdev\n",
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "def train_test_SVM(x_train_features, x_test_features, y_train, y_test):\n",
        "    tr_set = CDataset(x_train_features, y_train)\n",
        "    # Train the SVM\n",
        "    print(\"Build SVM\")\n",
        "    xval_splitter = CDataSplitterKFold()\n",
        "    clf_lin = CClassifierSVM()\n",
        "    xval_lin_params = {'C': [1]}\n",
        "    print(\"Find the best params\")\n",
        "    best_lin_params = clf_lin.estimate_parameters(\n",
        "        dataset=tr_set,\n",
        "        parameters=xval_lin_params,\n",
        "        splitter=xval_splitter,\n",
        "        metric='accuracy',\n",
        "        perf_evaluator='xval'\n",
        "    )\n",
        "    print(\"Finish Train\")\n",
        "    print(\"The best training parameters are: \", [\n",
        "          (k, best_lin_params[k]) for k in sorted(best_lin_params)])\n",
        "    print(\"Train SVM\")\n",
        "    clf_lin.fit(tr_set.X, tr_set.Y)\n",
        "\n",
        "    # Test the Classifier\n",
        "    ts_set = CDataset(x_test_features, y_test)\n",
        "    y_pred = clf_lin.predict(ts_set.X)\n",
        "    metric = CMetricAccuracy()\n",
        "    acc = metric.performance_score(y_true=ts_set.Y, y_pred=y_pred)\n",
        "    confusion_matrix = CMetricConfusionMatrix()\n",
        "    cm = confusion_matrix.performance_score(y_true=ts_set.Y, y_pred=y_pred)\n",
        "    print(\"Confusion Matrix: \")\n",
        "    print(cm)\n",
        "    return tr_set, ts_set, clf_lin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting secml\n",
            "  Downloading secml-0.15-py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from secml) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from secml) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from secml) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.7/dist-packages (from secml) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from secml) (1.1.0)\n",
            "Requirement already satisfied: Pillow>=6.2.1 in /usr/local/lib/python3.7/dist-packages (from secml) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from secml) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from secml) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->secml) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->secml) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->secml) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3->secml) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->secml) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->secml) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->secml) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->secml) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->secml) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->secml) (1.24.3)\n",
            "Installing collected packages: secml\n",
            "Successfully installed secml-0.15\n",
            "2022-04-15 03:30:52,413 - secml.settings - INFO - New `SECML_HOME_DIR` created: /root/secml-data\n",
            "2022-04-15 03:30:52,413 - secml.settings - INFO - New `SECML_HOME_DIR` created: /root/secml-data\n",
            "2022-04-15 03:30:52,417 - secml.settings - INFO - Default configuration file copied to: /root/secml-data/secml.conf\n",
            "2022-04-15 03:30:52,417 - secml.settings - INFO - Default configuration file copied to: /root/secml-data/secml.conf\n",
            "2022-04-15 03:30:52,424 - secml.settings - INFO - New `SECML_DS_DIR` created: /root/secml-data/datasets\n",
            "2022-04-15 03:30:52,424 - secml.settings - INFO - New `SECML_DS_DIR` created: /root/secml-data/datasets\n",
            "2022-04-15 03:30:52,432 - secml.settings - INFO - New `SECML_MODELS_DIR` created: /root/secml-data/models\n",
            "2022-04-15 03:30:52,432 - secml.settings - INFO - New `SECML_MODELS_DIR` created: /root/secml-data/models\n",
            "2022-04-15 03:30:52,440 - secml.settings - INFO - New `SECML_EXP_DIR` created: /root/secml-data/experiments\n",
            "2022-04-15 03:30:52,440 - secml.settings - INFO - New `SECML_EXP_DIR` created: /root/secml-data/experiments\n",
            "2022-04-15 03:30:52,449 - secml.settings - INFO - New `SECML_LOGS_DIR` created: /root/secml-data/logs\n",
            "2022-04-15 03:30:52,449 - secml.settings - INFO - New `SECML_LOGS_DIR` created: /root/secml-data/logs\n",
            "2022-04-15 03:30:52,454 - secml.settings - INFO - New `SECML_PYTORCH_DIR` created: /root/secml-data/pytorch-data\n",
            "2022-04-15 03:30:52,454 - secml.settings - INFO - New `SECML_PYTORCH_DIR` created: /root/secml-data/pytorch-data\n",
            "Build SVM\n",
            "Find the best params\n",
            "Finish Train\n",
            "The best training parameters are:  [('C', 1)]\n",
            "Train SVM\n",
            "Confusion Matrix: \n",
            "CArray([[483   1]\n",
            " [  3  92]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the code section below, we trained an SVM classifier using TFIDF values extracted. \\\\\n",
        "**Run the code block below:**"
      ],
      "metadata": {
        "id": "G0cZ1mqaRl7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_set, ts_set, clf_lin = train_test_SVM(x_train_features, x_test_features, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TcGiCZJRmfo",
        "outputId": "27a3c1dd-5afc-4b92-d11d-3430f4f2c3c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build SVM\n",
            "Find the best params\n",
            "Finish Train\n",
            "The best training parameters are:  [('C', 1)]\n",
            "Train SVM\n",
            "Confusion Matrix: \n",
            "CArray([[483   1]\n",
            " [  3  92]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rKjzZTCO5N2"
      },
      "source": [
        "## **5. PGD Attack**\n",
        "Our approach is based on successful adversarial perturbations made to model input features. We employ the Projected Gradient Descent (PGD) method as an attack method to modify the feature values for desirable adversarial examples in the feature domain. PGD method is considered as one of the most powerful first-order adversaries. PGD algorithm iteratively finds the needed changes with a constraint, *dmax*, which is the Euclidean distance to the original input indicating the allowed level of perturbations, to achieve the maximum loss in classification. In our approach, we run PGD over a set of spam emails in iterations and generate adversarial examples in the feature space. Then we test these modified featuer vectors to see whether they could successfully bypass the detection. \\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCvPfWA-P1OG"
      },
      "source": [
        "def pdg_attack(clf_lin, tr_set, ts_set, y_test, feature_names, nb_attack, dmax, lb, ub):\n",
        "\n",
        "    class_to_attack = 1\n",
        "    cnt = 0  # the number of success adversaril examples\n",
        "\n",
        "    ori_examples2_x = []\n",
        "    ori_examples2_y = []\n",
        "\n",
        "    for i in range(nb_attack):\n",
        "        # take a point at random being the starting point of the attack\n",
        "        idx_candidates = np.where(y_test == class_to_attack)\n",
        "        # select nb_init_pts points randomly in candidates and make them move\n",
        "        rn = np.random.choice(idx_candidates[0].size, 1)\n",
        "        x0, y0 = ts_set[idx_candidates[0][rn[0]],\n",
        "                        :].X, ts_set[idx_candidates[0][rn[0]], :].Y\n",
        "\n",
        "        x0 = x0.astype(float)\n",
        "        y0 = y0.astype(int)\n",
        "        x2 = x0.tondarray()[0]\n",
        "        y2 = y0.tondarray()[0]\n",
        "\n",
        "        ori_examples2_x.append(x2)\n",
        "        ori_examples2_y.append(y2)\n",
        "\n",
        "    # Perform adversarial attacks\n",
        "    noise_type = 'l2'  # Type of perturbation 'l1' or 'l2'\n",
        "    y_target = 0\n",
        "    # dmax = 0.09  # Maximum perturbation\n",
        "\n",
        "    # Bounds of the attack space. Can be set to `None` for unbounded\n",
        "    solver_params = {\n",
        "        'eta': 0.01,\n",
        "        'max_iter': 1000,\n",
        "        'eps': 1e-4}\n",
        "\n",
        "    # set lower bound and upper bound respectively to 0 and 1 since all features are Boolean\n",
        "    pgd_attack = CAttackEvasionPGD(\n",
        "        classifier=clf_lin,\n",
        "        double_init_ds=tr_set,\n",
        "        distance=noise_type,\n",
        "        dmax=dmax,\n",
        "        lb=lb, ub=ub,\n",
        "        solver_params=solver_params,\n",
        "        y_target=y_target\n",
        "    )\n",
        "\n",
        "    ad_examples_x = []\n",
        "    ad_examples_y = []\n",
        "    ad_index = []\n",
        "    cnt = 0\n",
        "    for i in range(len(ori_examples2_x)):\n",
        "        x0 = ori_examples2_x[i]\n",
        "        y0 = ori_examples2_y[i]\n",
        "        y_pred_pgd, _, adv_ds_pgd, _ = pgd_attack.run(x0, y0)\n",
        "        if y_pred_pgd.item() == 0:\n",
        "            cnt = cnt + 1\n",
        "            ad_index.append(i)\n",
        "\n",
        "        ad_examples_x.append(adv_ds_pgd.X.tondarray()[0])\n",
        "        ad_examples_y.append(y_pred_pgd.item())\n",
        "\n",
        "        attack_pt = adv_ds_pgd.X.tondarray()[0]\n",
        "    print(\"PGD attack successful rate:\", cnt / nb_attack)\n",
        "    startTime2 = time.time()\n",
        "    ori_examples2_x = np.array(ori_examples2_x)\n",
        "    ori_examples2_y = np.array(ori_examples2_y)\n",
        "    ad_examples_x = np.array(ad_examples_x)\n",
        "    ad_examples_y = np.array(ad_examples_y)\n",
        "\n",
        "    ori_dataframe = pd.DataFrame(ori_examples2_x, columns=feature_names)\n",
        "    ad_dataframe = pd.DataFrame(ad_examples_x, columns=feature_names)\n",
        "\n",
        "    # extract the success and fail examples\n",
        "    ad_dataframe['ad_label'] = ad_examples_y\n",
        "    ad_success = ad_dataframe.loc[ad_dataframe.ad_label == 0]\n",
        "    ori_success = ori_dataframe.loc[ad_dataframe.ad_label == 0]\n",
        "    ad_fail = ad_dataframe.loc[ad_dataframe.ad_label == 1]\n",
        "    ori_fail = ori_dataframe.loc[ad_dataframe.ad_label == 1]\n",
        "\n",
        "    ad_success_x = ad_success.drop(columns=['ad_label'])\n",
        "    ad_fail_x = ad_fail.drop(columns=['ad_label'])\n",
        "\n",
        "    result = (ad_success_x - ori_success)\n",
        "    ori_dataframe.to_csv('ori_dataframe.csv')\n",
        "    ad_dataframe.to_csv('ad_dataframe.csv')\n",
        "    result.to_csv('result.csv')\n",
        "    return result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, cnt/nb_attack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3qmM8uxrzrg"
      },
      "source": [
        "With the code section below, we run PGD attacks on the trained classifier with 100 spam emails and 0.06 dmax \\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RYKIvm0QMHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992866c3-6a5d-4d30-e6b1-5e0309a77b73"
      },
      "source": [
        "lb = np.ndarray.min(x_train_features.toarray())\n",
        "ub = np.ndarray.max(x_train_features.toarray())\n",
        "attack_amount = 100\n",
        "dmax = 0.06\n",
        "result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, successful_rate = pdg_attack(clf_lin, tr_set, ts_set, y_test, feature_names, attack_amount, dmax, lb, ub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD attack successful rate: 0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUx6FBQiCBr"
      },
      "source": [
        "## **6. Magical Words**\n",
        "Adversarial emails are crafted by adding “magic words” to the original spam emails. The “magic words” are identified by intersecting the unique ham words with the “top words”. Specifically,  the  unique  ham  words  are  the  word  that  only appeared  in  ham  emails  but  not in  spam  emails.  After the  PGD  attack on the set of spam emails,  we find which features are modified to the largest extent to bypass the detection. We then select the “top words” whose features have been changed the most by the PGD attack. (The changes are measured using the variance of TF-IDF differences before and after the PGD perturbation over these spam emails.) In  our  experiments,  we  use  the  top  100  words,  which  is relatively  efficient. This  set  is  relatively  small  and demonstrates a high success rate by using the magic words to fool the classifier. \\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc552xOTSQZn"
      },
      "source": [
        "def magical_word(x_train, x_test, y_train, y_test, result, cnt):\n",
        "    # Method 2\n",
        "    x2result1 = result\n",
        "    x2result1 = np.array(x2result1)\n",
        "    x2result = result\n",
        "    x2result = x2result.multiply(x2result1)\n",
        "\n",
        "    sum_number = x2result.sum() / cnt\n",
        "    sum_number = pd.DataFrame(sum_number, columns=['sum_number'])\n",
        "    sum_number = sum_number.sort_values(\n",
        "        by='sum_number', ascending=False, inplace=False)\n",
        "\n",
        "    sum_number_pd = pd.DataFrame(sum_number.index[:100])\n",
        "    sum_number_pd.to_csv(\"x2result.csv\")\n",
        "    d = {'message': x_train, 'label': y_train}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    d1 = {'message': x_test, 'label': y_test}\n",
        "    df1 = pd.DataFrame(data=d1)\n",
        "    frames = [df, df1]\n",
        "    messages = pd.concat(frames)\n",
        "    messages.to_csv(\"messages.csv\")\n",
        "    spam = messages[messages.label == 1]\n",
        "    ham = messages[messages.label == 0]\n",
        "\n",
        "    # Tf-idf for spam datasets\n",
        "    vect_spam = TfidfVectorizer()\n",
        "    vect_spam.fit_transform(spam['message'])\n",
        "    header_spam = vect_spam.get_feature_names_out()\n",
        "\n",
        "    # Tf-idf for ham datasets\n",
        "    vect_ham = TfidfVectorizer()\n",
        "    vect_ham.fit_transform(ham['message'])\n",
        "    header_ham = vect_ham.get_feature_names_out()\n",
        "\n",
        "    # find unique ham words\n",
        "    ham_unique = list(set(header_ham).difference(set(header_spam)))\n",
        "    header_ham1 = pd.DataFrame(ham_unique)\n",
        "    header_ham1.to_csv(\"ham_unique.csv\")\n",
        "\n",
        "    with open(\"x2result.csv\", \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        top100_features = []\n",
        "        for row in reader:\n",
        "            top100_features.append(row[1])\n",
        "    top100_features = top100_features[1:]\n",
        "    # in ham & top100\n",
        "\n",
        "    ham_unique_in_top = list(\n",
        "        set(ham_unique).intersection(set(top100_features)))\n",
        "    words14str = \"\"\n",
        "    for item in ham_unique_in_top:\n",
        "        words14str = words14str + \" \" + item\n",
        "    return words14str, spam, ham"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjldI2rTs_N5"
      },
      "source": [
        "With the code section below, we identifed a set of magic word given the successful perturbations. \\\\\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yC_GNgts8UO",
        "outputId": "f66aa7e9-b7d4-4587-b2a5-0376f330b8a6"
      },
      "source": [
        "words14str, spam, ham = magical_word(x_train, x_test, y_train, y_test, result, cnt)\n",
        "print(words14str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " benjamin discourse vt vowel ipa ohayosensei ammondt context phonetic workshop cascadilla gala grammar translation linguist bralich linguistic phonology chorus elra sentence risked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9e3VAwouiQK"
      },
      "source": [
        "## **7. Crafting Adversarial Emails & Attacking SVM**\n",
        "After we find the magical words, we then insert them back to the original spam emails. This proccess is what we called \"crafting adversarial emails\". Then, we feed the new feature vectors of these crafted emails to the SVM classifier to see if they would be misclassified as ham emails.  \\\\\n",
        "**Run the code block below:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxY8tJakw2yv"
      },
      "source": [
        "m2_empty = pd.DataFrame()\n",
        "spam_cnt = 0\n",
        "threads = []\n",
        "m2_empty_l1 = pd.DataFrame()\n",
        "m2_empty_l2 = pd.DataFrame()\n",
        "m2_empty_l3 = pd.DataFrame()\n",
        "m2_empty_l4 = pd.DataFrame()\n",
        "m2_list = [m2_empty_l1, m2_empty_l2, m2_empty_l3, m2_empty_l4]\n",
        "\n",
        "def single_transform(x, method, feature_model, feature_names, scaler, selection_model):\n",
        "  result = feature_model.transform(x)\n",
        "  if selection_model != 'NaN':\n",
        "    result = selection_model.transform(result)\n",
        "  return result\n",
        "\n",
        "class myThread(threading.Thread):\n",
        "\n",
        "    def __init__(self, threadID, name, spam_message, words14str, method, feature_model, feature_names, scaler, clf_lin, list_index, selection_model):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.threadID = threadID\n",
        "        self.name = name\n",
        "        self.spam_message = spam_message\n",
        "        self.words14str = words14str\n",
        "        self.method = method\n",
        "        self.feature_model = feature_model\n",
        "        self.feature_names = feature_names\n",
        "        self.scaler = scaler\n",
        "        self.clf_lin = clf_lin\n",
        "        self.list_index = list_index\n",
        "        self.lock = threading.Lock()\n",
        "        self.selection_model = selection_model\n",
        "\n",
        "    def run(self):\n",
        "        global spam_cnt\n",
        "        print(\"Starting \" + self.name)\n",
        "        spam_cnt_1 = m2_empty_out(self.name, self.spam_message, self.words14str, self.method,\n",
        "                                  self.feature_model, self.feature_names, self.scaler, self.clf_lin,\n",
        "                                  self.list_index, self.selection_model)\n",
        "        spam_cnt = spam_cnt+spam_cnt_1\n",
        "        time.sleep(0.1)\n",
        "        print(\"Exiting \" + self.name)\n",
        "\n",
        "\n",
        "def m2_empty_out(name, spam_message, words14str, method, feature_model, feature_names, scaler, clf_lin, list_index, selection_model):\n",
        "    m2_empty_1 = pd.DataFrame()\n",
        "    spam_cnt_1 = 0\n",
        "    global m2_list\n",
        "\n",
        "    for j in spam_message.message:\n",
        "        choose_email = [j + words14str]\n",
        "        message_14_email = pd.DataFrame(choose_email, columns=[\"message\"])\n",
        "        message_14_tf_idf = single_transform(\n",
        "            message_14_email[\"message\"], method, feature_model, feature_names, scaler, selection_model)\n",
        "        message_14_tf_idf = pd.DataFrame(\n",
        "            message_14_tf_idf.toarray(), columns=feature_names)\n",
        "        message_14_y = [1]\n",
        "        message_14_y = pd.Series(message_14_y)\n",
        "        message_CData = CDataset(message_14_tf_idf, message_14_y)\n",
        "        message_14_pred = clf_lin.predict(message_CData.X)\n",
        "\n",
        "        if message_14_pred == 0:\n",
        "            spam_cnt_1 = spam_cnt_1 + 1\n",
        "            m2_empty_1 = m2_empty_1.append(\n",
        "                message_14_tf_idf, ignore_index=True)\n",
        "\n",
        "    m2_list[list_index] = m2_list[list_index].append(\n",
        "        m2_empty_1, ignore_index=True)\n",
        "\n",
        "    return spam_cnt_1\n",
        "\n",
        "\n",
        "\n",
        "def svm_attack(method, clf_lin, spam, words14str, feature_model, feature_names, scaler, selection_model):\n",
        "\n",
        "    global m2_empty\n",
        "\n",
        "    spam_messages = np.array_split(spam, 4)\n",
        "    print(\"Start processing message\")\n",
        "    thread1 = myThread(1, \"Thread-1\", spam_messages[0], words14str,\n",
        "                       method, feature_model, feature_names, scaler, clf_lin, 0, selection_model)\n",
        "    thread2 = myThread(2, \"Thread-2\", spam_messages[1], words14str,\n",
        "                       method, feature_model, feature_names, scaler, clf_lin, 1, selection_model)\n",
        "    thread3 = myThread(3, \"Thread-3\", spam_messages[2], words14str,\n",
        "                       method, feature_model, feature_names, scaler, clf_lin, 2, selection_model)\n",
        "    thread4 = myThread(4, \"Thread-4\", spam_messages[3], words14str,\n",
        "                       method, feature_model, feature_names, scaler, clf_lin, 3, selection_model)\n",
        "    threads.append(thread1)\n",
        "    threads.append(thread2)\n",
        "    threads.append(thread3)\n",
        "    threads.append(thread4)\n",
        "    for t in threads:\n",
        "        t.start()\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    m2_empty = m2_empty.append(m2_list[0], ignore_index=True)\n",
        "    m2_empty = m2_empty.append(m2_list[1], ignore_index=True)\n",
        "    m2_empty = m2_empty.append(m2_list[2], ignore_index=True)\n",
        "    m2_empty = m2_empty.append(m2_list[3], ignore_index=True)\n",
        "\n",
        "    print(\"Exiting Main Thread\")\n",
        "    print('White box attack with length on SVM:')\n",
        "    print('Number of samples provided:', len(spam))\n",
        "    print('Number of crafted sample that got misclassified:', spam_cnt)\n",
        "    print('Successful rate:', spam_cnt / len(spam))\n",
        "\n",
        "    return m2_empty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raxGc9-UsT6d"
      },
      "source": [
        "With the code section below, we crafted a set of spam emails and feed them back to the trained classifier to see if they can bypass. \\\\\n",
        "Run the code block below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo5YVW5zw0Iu",
        "outputId": "d8074d83-63dc-40ab-dad5-720a35252b27"
      },
      "source": [
        "m2_empty = svm_attack('TFIDF', clf_lin, spam,words14str, feature_model, feature_names, scalar, 'NaN')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start processing message\n",
            "Starting Thread-1Starting Thread-2\n",
            "\n",
            "Starting Thread-3Starting Thread-4\n",
            "\n",
            "Exiting Thread-3\n",
            "Exiting Thread-1\n",
            "Exiting Thread-4\n",
            "Exiting Thread-2\n",
            "Exiting Main Thread\n",
            "White box attack with length on SVM:\n",
            "Number of samples provided: 481\n",
            "Number of crafted sample that got misclassified: 186\n",
            "Successful rate: 0.3866943866943867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tasks**\n",
        "### **Task 1** ### \n",
        "Integrate the step 1 - 7 above into one function in the below code block.\n",
        "This function should only have two inputs, with the first being the method we use for feature extraction, and the dmax we would use for PGD attacks. This function should return the set of magic word identified and print out the success rate for step 7. \\\\\n",
        "Hint: You can change the method of feature extraction by changing the value of the \"method\" variable."
      ],
      "metadata": {
        "id": "-KIQP6h8VzVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def all_in_one(FE_method, PGD_dmax):\n",
        "  # inject your code here:\n",
        "  return magic_word"
      ],
      "metadata": {
        "id": "penr97JhaHgq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2** ###\n",
        "Using the function you wrote for task 1, run it for 5 times with dmax being 0.02, 0.04, 0.06, 0.08, and 0.1 respectively for each time the feature extraction method being TF-IDF, modified word2vec, and modified doc2vec. Record the magic word attack success rate and the magic word for each time and fill in the table below by changing the \"dmax =\" with the actual success rate:"
      ],
      "metadata": {
        "id": "34D9Gts3bKTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TF-IDF  | Word2vec | Doc2vec\n",
        "-------------------|------------------|------------------\n",
        "dmax = 0.02| dmax = 0.02 | dmax = 0.02\n",
        "dmax = 0.04| dmax = 0.04 | dmax = 0.04\n",
        "dmax = 0.06| dmax = 0.06 | dmax = 0.06\n",
        "dmax = 0.08| dmax = 0.08 | dmax = 0.08\n",
        "dmax = 0.1| dmax = 0.1 | dmax = 0.1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JcGletlYgMHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 3** ###\n",
        "Draw a graph with the x axis being dmax and the y axis being success rates, and answer the questions below: \\\\\n",
        "\n",
        "\n",
        "1.   Which feature extraction method has the single highest magic word attack success rate?\n",
        "2.   Which feature extraction method do you think is better for magic word attack and why?\n",
        "\n"
      ],
      "metadata": {
        "id": "_gzZc4jRiclT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for drawing graph"
      ],
      "metadata": {
        "id": "8F1_bMUJjgre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answers to the two questions"
      ],
      "metadata": {
        "id": "jOIYXQ2Vjk6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 4** ###\n",
        "Do the following:\n",
        "1. Select a set of maigc word with the highest success rate in task 2\n",
        "2. Train an KNN classifier with dataset provided and the same featuer extraction method you used to obtain the set of magic word you selected. You can use any parameter for the KNN classifier.\n",
        "3. Pick 5 spam emails and add the magic words to the end of them. Convert them back to feature vectors and feed them to the KNN you trained. How many of the 5 spam emails got missclassified?"
      ],
      "metadata": {
        "id": "IpuBzQoSkQKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Inject your code here"
      ],
      "metadata": {
        "id": "G2ACFQv_mPyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}